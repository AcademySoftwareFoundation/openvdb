{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JaggedTensor and GridBatch\n",
    "There are two fundamental classes in fVDB you will encounter frequently: `JaggedTensor` and `GridBatch`.  \n",
    "\n",
    "```python\n",
    "fvdb.JaggedTensor\n",
    "fvdb.GridBatch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridBatch\n",
    "\n",
    "A `GridBatch` is an indexing structure which maps 3D `ijk` coordinates to integer offsets which can be used to look up attributes in a tensor.  The figure below illustrates this process for a GridBatch containing a single grid.\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/gridbatch.png\"  alt=\"Image Index Grid\" width=\"800\"/>>\n",
    "\n",
    "</center>\n",
    "\n",
    "In practice, `GridBatch` is an ordered collection of 3D grids.  Specifically these 3D grids are **NanoVDB** grids of the special `IndexGrid` type which only stores a unique 'index' integer value at each active voxel location.  This 'index' is an offset into some external data array, a tensor, (one that is not contained in the `IndexGrid`/`GridBatch`) where contiguous array members correspond to spatially nearby voxels.  `IndexGrid` will allow us to reference into this 'sidecar' tensor of attribute data given a spatial `ijk` set of coordinates.\n",
    "\n",
    "Each `IndexGrid` member in a `GridBatch` can have different topologies, different numbers of active voxels and different voxel dimensions and origins per-grid.\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/gridbatch_concept.svg\"  alt=\"Image Index Grid\" width=\"800\"/>>\n",
    "\n",
    "</center>\n",
    "\n",
    "##### Images as 2D GridBatch\n",
    "\n",
    "\n",
    "To help explain these concepts, let's consider how we might treat image data with this framework (an image can be thought of as a dense 2D grid after all).  If an image is a 2D grid of pixels, we could imagine the position of each pixel could be expressed as `i,j` coordinates.  For an RGB image, each pixel would contain 3 associated values (R,G,B).  \n",
    "\n",
    "In this way, we could decompose an image into an `IndexGrid` of `i,j` coordinates and flatten the RGB values into a list of size `[Number of Pixels, 3]` (the features for our attribute tensor).  The `i,j` coordinates can be used with the `IndexGrid` to index into the list of RGB values to retrieve the RGB value for each pixel.  \n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/image_index_grid_diagram.svg\"  alt=\"Image Index Grid\" width=\"1000\"/>>\n",
    "\n",
    "</center>\n",
    "\n",
    "If we had many images, each of different sizes, we can imagine constructing a `GridBatch` as an ordered set of their `IndexGrid`s.  All the RGB values would go into a sidecar attribute tensor of *jagged* features where each feature array would have to be of a different length corresponding to the image size.  This is the essence of the relationship between `GridBatch` and `JaggedTensor` in fVDB, but more on `JaggedTensor` later…\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/grid_batch_diagram.svg\"  alt=\"Grid Batch\" width=\"600\"/>>\n",
    "\n",
    "</center>\n",
    "\n",
    "Lastly, it is important to know that each grid in the `GridBatch` will be on the same device and processed together by operators in mini-batch-like fashion.  \n",
    "\n",
    "Let's put together our first `GridBatch` to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual suspects and fvdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import fvdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid 0 has 565 voxels, \n",
      "        voxel size of [0.4180232286453247, 0.9078353047370911, 0.7569453120231628] \n",
      "        and an origin of [0.053342342376708984, 0.666057825088501, 0.5568195581436157]\n",
      "Grid 1 has 505 voxels, \n",
      "        voxel size of [0.4231018126010895, 0.06702223420143127, 0.027880029752850533] \n",
      "        and an origin of [0.23663270473480225, 0.9503206014633179, 0.9787915945053101]\n",
      "Grid 2 has 840 voxels, \n",
      "        voxel size of [0.552473783493042, 0.6610133647918701, 0.17690575122833252] \n",
      "        and an origin of [0.19191621243953705, 0.4792993664741516, 0.8062528967857361]\n",
      "Grid 3 has 192 voxels, \n",
      "        voxel size of [0.3558811545372009, 0.006604234222322702, 0.05409170687198639] \n",
      "        and an origin of [0.7635682821273804, 0.5538259744644165, 0.6306257247924805]\n",
      "Grid 4 has 374 voxels, \n",
      "        voxel size of [0.21053555607795715, 0.037602122873067856, 0.6572714447975159] \n",
      "        and an origin of [0.07630939781665802, 0.2442461997270584, 0.3808997869491577]\n",
      "Grid 5 has 707 voxels, \n",
      "        voxel size of [0.2730364203453064, 0.523075520992279, 0.2063577026128769] \n",
      "        and an origin of [0.34540674090385437, 0.4302160143852234, 0.25126296281814575]\n",
      "Grid 6 has 162 voxels, \n",
      "        voxel size of [0.740619421005249, 0.06447814404964447, 0.2899278998374939] \n",
      "        and an origin of [0.380511999130249, 0.75043785572052, 0.5695812106132507]\n",
      "Grid 7 has 613 voxels, \n",
      "        voxel size of [0.541886568069458, 0.4727923572063446, 0.2005639523267746] \n",
      "        and an origin of [0.48677605390548706, 0.0562465600669384, 0.717363178730011]\n",
      "Grid 0, feature Index at ijk [[394, -91, 474]] : 401\n",
      "Grid 1, feature Index at ijk [[211, 65, -404]] : 962\n",
      "Grid 2, feature Index at ijk [[-156, -225, -111]] : 1141\n",
      "Grid 3, feature Index at ijk [[198, 176, -362]] : 2063\n",
      "Grid 4, feature Index at ijk [[412, -42, 83]] : 2375\n",
      "Grid 5, feature Index at ijk [[-398, 256, 50]] : 2777\n",
      "Grid 6, feature Index at ijk [[291, 193, -159]] : 3310\n",
      "Grid 7, feature Index at ijk [[427, 121, -350]] : 3876\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "# Randomly generate different numbers of voxels we desire in each grid in our batch\n",
    "num_voxels_per_grid = [np.random.randint(100, 1_000) for _ in range(batch_size)]\n",
    "\n",
    "# A list of randomly generated 3D indices for each grid in our batch in the range [-512, 512]\n",
    "ijks = [torch.randint(-512, 512, (num_voxels_per_grid[i], 3), device='cuda') for i in range(batch_size)]\n",
    "\n",
    "# Create an fvdb.GridBatch from the list of indices!!\n",
    "grid = fvdb.sparse_grid_from_ijk(fvdb.JaggedTensor(ijks), # We'll explain JaggedTensor in a moment…\n",
    "                                 voxel_sizes = [np.random.rand(3) for _ in range(batch_size)], # Random, different voxel sizes for each grid in our batch\n",
    "                                 origins     = [np.random.rand(3) for _ in range(batch_size)], # Random, different grid origins for each grid in our batch\n",
    "                                )\n",
    "\n",
    "# This grid will be on the GPU because the `ijks` were on that device\n",
    "assert(grid.device == ijks[0].device == torch.device('cuda:0'))\n",
    "\n",
    "# Each member of the batch has different voxel size dimensions, a different origin in space and different number of voxels\n",
    "for i in range(grid.grid_count):\n",
    "    print(f\"\"\"Grid {i} has {grid.num_voxels_at(i)} voxels,\n",
    "        voxel size of {grid.voxel_size_at(i).tolist()}\n",
    "        and an origin of {grid.origin_at(i).tolist()}\"\"\")\n",
    "\n",
    "# NOTE: This GridBatch (Batch of IndexGrids) just expresses the topology of the grids and can be used to reference a sidecar flat array of features but we won't create this sidecar in this example…\n",
    "# We can get the index into this hypothetical sidecar feature array with any `ijk` coordinate (if we ask for an `ijk` not in the grid, -1 is given as the index)\n",
    "\n",
    "# Let's retrieve a random ijk coordinate from each of the lists we used to make the grids\n",
    "ijk_queries = fvdb.JaggedTensor([ijks[n][np.random.randint(len(ijks[n]))][None,:] for n  in range(grid.grid_count)])\n",
    "# Use the GridBatch to get indices into the sidecar feature array from the `ijk` coordinate in each grid\n",
    "feature_indices = grid.ijk_to_index(ijk_queries)\n",
    "for i, (ijk,i_f) in enumerate(zip(ijk_queries, feature_indices)):\n",
    "    print(f\"Grid {i}, feature Index at ijk {ijk.jdata.tolist()} : {i_f.jdata.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more convenient ways that fVDB provides to create a `GridBatch` besides building from lists of coordinates.  Please see the fVDB documentation for more useful examples for creating a `GridBatch` from pointclouds, meshes or dense tensors, to name a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JaggedTensor\n",
    "\n",
    "`JaggedTensor` is the supporting feature data that is paired with a `GridBatch`.  \n",
    "\n",
    "You can think of `JaggedTensor` as an ordered list of PyTorch Tensors, one for each grid in the `GridBatch`.  Same as `GridBatch`, the Tensors are all on the same device and processed together in a mini-batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a GridBatch a matching JaggedTensor can be created from a Tensor of shape `(total_voxels, feature_dim)`\n",
    "#   Here we create a JaggedTensor with 8 features (an arbitrary choice of number of features for our example) of random values for our GridBatch\n",
    "features = grid.jagged_like(torch.randn(grid.total_voxels, 8, device=grid.device))\n",
    "\n",
    "# This would have the equivalent effect:\n",
    "features  = fvdb.JaggedTensor([torch.randn(grid.num_voxels[i], 8, device=grid.device) for i in range(grid.grid_count)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `JaggedTensor` above, you can see how we constructed it with a list of heterogeneously sized Tensors whose shapes were of the form\n",
    "`[ [B1, C], [B2, C], [B3, C], …] ]` where the value of `B[#]` would be the number of active voxels in each grid of the `GridBatch` and `C` is the number of feature channels (8 was chosen in this case).\n",
    "\n",
    "Note how each Tensor element in our `JaggedTensor` can have different numbers of active voxels (similar to `GridBatch`) but the same number of per-voxel feature channels.  This is distinctly different from the classic representation of 3D data in a PyTorch `Tensor` which is usually a homogeneously shaped Tensor of shape `[B, C, H, W, D]` where `B` would be the number of \"grids\" in our batch, `C` is the number of feature channels, and `H, W, D` are the 3 index dimensions of a dense grid.\n",
    "\n",
    "Internally, `JaggedTensor` does not represent the list of features as a list of differently sized PyTorch tensors, but instead stores a single tensor of the features accompanied by a special index structure that allows us to access the feature data for each grid in the `GridBatch`.  \n",
    "\n",
    "The `jdata` attribute of a `JaggedTensor` contains all the feature data values in this single list.  `jdata` is a `Tensor` of shape `[N, C]` where `N` is the total number of active voxels in the batch and `C` is the number of feature channels.  \n",
    "\n",
    "`jdata`'s shape would be equivalent to the result of concatenating the list of heterogeneously sized Tensors, mentioned above, along their first axis into a single Tensor whose shape would be `[B1+B2+B3…+Bn, C]`.\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src = \"img/jdata.jpg\" width=1200 alt=\"jdata\">\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of features data for the entire GridBatch: torch.Size([3958, 8])\n"
     ]
    }
   ],
   "source": [
    "print(f\"size of features data for the entire GridBatch: {features.jdata.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine which grid each feature belongs to, `JaggedTensor` contains indexing information in its `jidx` attribute.\n",
    "\n",
    "`jidx` is a `Tensor` of shape `[N]` where `N` is again the total number of active voxels across the batch.  Each element of `jidx` is an integer that tells us which grid in the `GridBatch` the corresponding feature in `jdata` belongs to.  The grid membership in `jidx` is ordered starting from 0 and members of the same batch are contiguous.\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src = \"img/jidx.jpg\" width=1200 alt=\"jdata\">\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per-feature membership for each voxel in the GridBatch: tensor([0, 0, 0,  ..., 7, 7, 7], device='cuda:0', dtype=torch.int16)\n",
      "\n",
      "the size of the features of the 4th grid in this batch: torch.Size([192, 8])\n"
     ]
    }
   ],
   "source": [
    "print(f\"per-feature membership for each voxel in the GridBatch: {features.jidx}\")\n",
    "print(f\"\\nthe size of the features of the 4th grid in this batch: {features.jdata[features.jidx==3].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, `JaggedTensor` has a `joffsets` attribute that can also be used to index into `jdata` to get the feature data for each grid in the batch.\n",
    "\n",
    "The `joffsets` attribute is a `Tensor` of shape `[B, 2]` which has the start and end offset into `jdata` that corresponds to each grid in the batch.  This is essentially the same information that can be found in `jidx` but expressed in a different form.\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src = \"img/joffsets.jpg\" width=1200 alt=\"jdata\">\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per-grid offsets into the feature data:\n",
      "tensor([[   0,  565],\n",
      "        [ 565, 1070],\n",
      "        [1070, 1910],\n",
      "        [1910, 2102],\n",
      "        [2102, 2476],\n",
      "        [2476, 3183],\n",
      "        [3183, 3345],\n",
      "        [3345, 3958]], device='cuda:0')\n",
      "\n",
      "the size of the features of the 4th grid in this batch: torch.Size([192, 8])\n"
     ]
    }
   ],
   "source": [
    "print(\"per-grid offsets into the feature data:\")\n",
    "print(features.joffsets)\n",
    "print(f\"\\nthe size of the features of the 4th grid in this batch: {features.jdata[features.joffsets[3][0]:features.joffsets[3][1]].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features stored in a `JaggedTensor` can be of any type that PyTorch supports, including float, float64, float16, int, bool, etc., and we can have an arbitrary number of feature channels per voxel.  \n",
    "\n",
    "For instance, there could be a `JaggedTensor` with 1 float feature that represents a signed distance field in each grid, or 3 float features that represent some RGB color in each voxel of the grids, or a 192 float feature that represents a learned feature vector of each voxel in each grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single scalar feature\n",
    "features = grid.jagged_like(torch.randn(grid.total_voxels, 1, dtype=torch.float, device=grid.device))\n",
    "\n",
    "# Cast to a double\n",
    "features = features.double()\n",
    "\n",
    "# A JaggedTensor of 192 float features\n",
    "features = grid.jagged_like(torch.randn(grid.total_voxels, 192, dtype=torch.float, device=grid.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fvdb_exec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
